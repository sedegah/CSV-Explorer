
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f02e310",
   "metadata": {},
   "source": [
    "# CSV Smart Summary Tool\n",
    "*Automatic Data Exploration and Insight Generation*\n",
    "\n",
    "This notebook automatically analyzes any uploaded CSV dataset and generates comprehensive statistical summaries, visualizations, correlations, and natural-language insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864ba04",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e95fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1ab95",
   "metadata": {},
   "source": [
    "## Section 2: Define CSV Upload and Parsing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2825d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"CSV loaded successfully!\")\n",
    "        print(f\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "        print(f\"\\nColumn types detected:\")\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            print(f\"  - {col}: {dtype}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def categorize_columns(df):\n",
    "    return {\n",
    "        'numerical': df.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "        'categorical': df.select_dtypes(include=['object', 'category']).columns.tolist(),\n",
    "        'datetime': df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d11772",
   "metadata": {},
   "source": [
    "## Section 3: Implement Column-Level Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pct(series):\n",
    "    return f\"{(series.isna().sum() / len(series)) * 100:.2f}%\"\n",
    "\n",
    "def summarize_numerical_columns(df, numerical_cols):\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'Column': col,\n",
    "            'Count': df[col].count(),\n",
    "            'Mean': df[col].mean(),\n",
    "            'Median': df[col].median(),\n",
    "            'Std Dev': df[col].std(),\n",
    "            'Min': df[col].min(),\n",
    "            'Max': df[col].max(),\n",
    "            'Missing %': get_missing_pct(df[col])\n",
    "        }\n",
    "        for col in numerical_cols\n",
    "    ])\n",
    "\n",
    "def summarize_categorical_columns(df, categorical_cols):\n",
    "    data = []\n",
    "    for col in categorical_cols:\n",
    "        value_counts = df[col].value_counts()\n",
    "        top_value = value_counts.index[0] if len(value_counts) > 0 else None\n",
    "        top_count = value_counts.values[0] if len(value_counts) > 0 else 0\n",
    "        data.append({\n",
    "            'Column': col,\n",
    "            'Unique Values': df[col].nunique(),\n",
    "            'Top Value': top_value,\n",
    "            'Top Count': top_count,\n",
    "            'Missing %': get_missing_pct(df[col])\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def display_column_summaries(df, col_categories):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLUMN-LEVEL SUMMARIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if col_categories['numerical']:\n",
    "        print(\"\\nNUMERICAL COLUMNS:\")\n",
    "        print(\"-\"*80)\n",
    "        display(summarize_numerical_columns(df, col_categories['numerical']))\n",
    "    \n",
    "    if col_categories['categorical']:\n",
    "        print(\"\\nCATEGORICAL COLUMNS:\")\n",
    "        print(\"-\"*80)\n",
    "        display(summarize_categorical_columns(df, col_categories['categorical']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f15b65",
   "metadata": {},
   "source": [
    "## Section 4: Create Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816eb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distributions(df, col_categories):\n",
    "    numerical_cols = col_categories['numerical']\n",
    "    categorical_cols = col_categories['categorical']\n",
    "    \n",
    "    if numerical_cols:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NUMERICAL DISTRIBUTIONS (Histograms & Box Plots)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        n_cols = min(len(numerical_cols), 3)\n",
    "        n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "        \n",
    "        for idx, col in enumerate(numerical_cols):\n",
    "            axes[idx].hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        for idx in range(len(numerical_cols), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(numerical_cols), figsize=(5*len(numerical_cols), 5))\n",
    "        axes = np.atleast_1d(axes)\n",
    "        \n",
    "        for idx, col in enumerate(numerical_cols):\n",
    "            axes[idx].boxplot(df[col].dropna())\n",
    "            axes[idx].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_ylabel(col)\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CATEGORICAL DISTRIBUTIONS (Bar Charts)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            top_values = df[col].value_counts().head(10)\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            top_values.plot(kind='bar', color='coral', edgecolor='black', alpha=0.7, ax=ax)\n",
    "            ax.set_title(f'Top 10 Categories in {col}', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326df55c",
   "metadata": {},
   "source": [
    "## Section 5: Compute and Visualize Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866673c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strong_correlations(corr_matrix, threshold=0.7):\n",
    "    strong_corrs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_value) >= threshold:\n",
    "                strong_corrs.append({\n",
    "                    'Variable 1': corr_matrix.columns[i],\n",
    "                    'Variable 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_value\n",
    "                })\n",
    "    return strong_corrs\n",
    "\n",
    "def compute_correlations(df, numerical_cols):\n",
    "    if len(numerical_cols) < 2:\n",
    "        print(\"Not enough numerical columns to compute correlations.\")\n",
    "        return None\n",
    "    \n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    display(corr_matrix.round(3))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                fmt='.2f', square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ef829",
   "metadata": {},
   "source": [
    "## Section 6: Generate Natural-Language Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba68549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(df, col_categories, corr_matrix=None):\n",
    "    insights = []\n",
    "    insights.append(f\"Dataset Overview: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    total_missing = df.isna().sum().sum()\n",
    "    if total_missing > 0:\n",
    "        missing_pct = (total_missing / (df.shape[0] * df.shape[1])) * 100\n",
    "        insights.append(f\"Missing Data: {total_missing} missing values ({missing_pct:.2f}% of total)\")\n",
    "    else:\n",
    "        insights.append(\"Data Quality: No missing values detected\")\n",
    "    \n",
    "    if col_categories['numerical']:\n",
    "        insights.append(f\"\\nNumerical Features: {len(col_categories['numerical'])} columns detected\")\n",
    "        for col in col_categories['numerical']:\n",
    "            skewness = df[col].skew()\n",
    "            if abs(skewness) > 1:\n",
    "                skew_type = \"highly left-skewed\" if skewness < 0 else \"highly right-skewed\"\n",
    "                insights.append(f\"  - {col} is {skew_type} (skewness: {skewness:.2f})\")\n",
    "            \n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "            if outliers > 0:\n",
    "                outlier_pct = (outliers / df[col].count()) * 100\n",
    "                insights.append(f\"  - {col} contains {outliers} outliers ({outlier_pct:.1f}% of data)\")\n",
    "    \n",
    "    if col_categories['categorical']:\n",
    "        insights.append(f\"\\nCategorical Features: {len(col_categories['categorical'])} columns detected\")\n",
    "        for col in col_categories['categorical']:\n",
    "            unique_count = df[col].nunique()\n",
    "            insights.append(f\"  - {col} has {unique_count} unique values\")\n",
    "    \n",
    "    if corr_matrix is not None:\n",
    "        strong_corrs = extract_strong_correlations(corr_matrix, threshold=0.7)\n",
    "        if strong_corrs:\n",
    "            insights.append(f\"\\nStrong Correlations (|r| > 0.7):\")\n",
    "            for corr in strong_corrs[:5]:\n",
    "                insights.append(f\"  - {corr['Variable 1']} -- {corr['Variable 2']}: {corr['Correlation']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NATURAL-LANGUAGE INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\".join(insights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b9932",
   "metadata": {},
   "source": [
    "## Section 7: Create Main Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_csv(file_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CSV SMART SUMMARY TOOL - Analysis Started\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n[Step 1/6] Loading CSV file...\")\n",
    "    df = load_csv(file_path)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n[Step 2/6] Categorizing columns...\")\n",
    "    col_categories = categorize_columns(df)\n",
    "    \n",
    "    print(\"\\n[Step 3/6] Generating column summaries...\")\n",
    "    display_column_summaries(df, col_categories)\n",
    "    \n",
    "    print(\"\\n[Step 4/6] Creating distribution visualizations...\")\n",
    "    visualize_distributions(df, col_categories)\n",
    "    \n",
    "    print(\"\\n[Step 5/6] Computing correlations...\")\n",
    "    corr_matrix = compute_correlations(df, col_categories['numerical']) if col_categories['numerical'] else None\n",
    "    \n",
    "    print(\"\\n[Step 6/6] Generating insights...\")\n",
    "    generate_insights(df, col_categories, corr_matrix)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Analysis Complete!\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82580a47",
   "metadata": {},
   "source": [
    "## Section 8: Test with Sample CSV Data\n",
    "\n",
    "Below we'll create a sample dataset and test the complete analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sample_data = {\n",
    "    'Age': np.random.randint(18, 80, 150),\n",
    "    'Income': np.random.normal(50000, 20000, 150),\n",
    "    'Experience_Years': np.random.randint(0, 40, 150),\n",
    "    'Department': np.random.choice(['Sales', 'Engineering', 'Marketing', 'HR', 'Finance'], 150),\n",
    "    'Education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 150),\n",
    "    'Performance_Score': np.random.uniform(1, 5, 150),\n",
    "}\n",
    "\n",
    "sample_data['Income'] = sample_data['Income'] + (sample_data['Experience_Years'] * 1000)\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "sample_df.loc[np.random.choice(sample_df.index, 5, replace=False), 'Income'] = np.nan\n",
    "sample_df.loc[np.random.choice(sample_df.index, 3, replace=False), 'Performance_Score'] = np.nan\n",
    "\n",
    "sample_csv_path = '/tmp/sample_employee_data.csv'\n",
    "sample_df.to_csv(sample_csv_path, index=False)\n",
    "\n",
    "print(f\"Sample dataset created: {sample_csv_path}\")\n",
    "print(f\"Shape: {sample_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(sample_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d393aee",
   "metadata": {},
   "source": [
    "### Run Complete Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_csv(sample_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241fe5d",
   "metadata": {},
   "source": [
    "## How to Use This Tool\n",
    "\n",
    "### For Your Own Dataset:\n",
    "\n",
    "1. **Replace the sample data**: In the \"Create Sample Dataset\" cell, replace the file path with your own CSV file:\n",
    "   ```python\n",
    "   analyze_csv('path/to/your/file.csv')\n",
    "   ```\n",
    "\n",
    "2. **Supported features**:\n",
    "   - Automatic detection of numerical and categorical columns\n",
    "   - Comprehensive statistical summaries\n",
    "   - Distribution visualizations (histograms, box plots, bar charts)\n",
    "   - Correlation analysis with heatmaps\n",
    "   - Natural-language insights about your data\n",
    "\n",
    "3. **Missing values**: The tool gracefully handles missing values and reports them\n",
    "\n",
    "### Customization:\n",
    "\n",
    "- Modify `threshold` parameter in `extract_strong_correlations()` to change correlation sensitivity\n",
    "- Adjust visualization parameters (e.g., `figsize`, `bins`, color schemes)\n",
    "- Add new insight generation logic in `generate_insights()` function\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Automated: One-line analysis of any CSV\n",
    "- Comprehensive: Covers statistics, distributions, correlations, and insights\n",
    "- Visual: Rich plots and heatmaps for pattern discovery\n",
    "- Insightful: Generates readable, actionable insights\n",
    "- Robust: Handles missing values and various data types"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
